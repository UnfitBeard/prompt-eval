# ğŸ§  Prompt-Eval â€” AI-Powered Prompt Evaluation Platform

This project is a full-stack application that allows users to **evaluate, score, and manage prompt templates** for various domains such as software engineering, education, and content writing.
It integrates three major components:

- ğŸŒ **Frontend:** Angular (19) â€” Modern UI for users and admins.
- ğŸ›  **Backend:** Node.js + Express â€” Authentication, API endpoints, and business logic.
- ğŸ¤– **AI Model:** Python + scikit-learn â€” Machine learning scoring engine (served separately).

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ AI_model/                # Python scoring model and artifacts
â”‚   â”œâ”€â”€ scoring_artifacts/   # Trained model, metadata & inference script
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â”œâ”€â”€ DOCKERFILE           # Optional containerization for model
â”‚   â””â”€â”€ my_venv/             # Python virtual environment (local)
â”‚
â”œâ”€â”€ backend/                 # Node.js + Express REST API
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ Controllers/
â”‚   â”‚   â”‚   â”œâ”€â”€ Auth/        # Login & registration logic
â”‚   â”‚   â”‚   â””â”€â”€ Prompts/    # Template & evaluation endpoints
â”‚   â”‚   â”œâ”€â”€ Models/         # Mongoose schemas
â”‚   â”‚   â”œâ”€â”€ Routes/         # Express route definitions
â”‚   â”‚   â””â”€â”€ Utils/          # Helper functions (e.g. JWT)
â”‚   â”œâ”€â”€ .env                # Environment variables
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ server.ts           # Entry point
â”‚
â””â”€â”€ promptEval/              # Angular Frontend (v19)
    â”œâ”€â”€ src/app/Components/  # UI components (login, dashboard, templates, etc.)
    â”œâ”€â”€ src/app/Guards/     # Auth guards for routes
    â”œâ”€â”€ src/app/Services/   # HTTP services for API & AI calls
    â”œâ”€â”€ angular.json
    â”œâ”€â”€ package.json
    â””â”€â”€ README.md
```

---

## âš¡ 1. Frontend â€” Angular

### ğŸ§­ Features

- ğŸ” Authentication (Login/Register)
- ğŸ§° Admin dashboard to manage templates
- ğŸ“ Template library with filters, ratings, and copy-to-clipboard
- ğŸ” Full-text search and filtering by difficulty, rating, and domain

### â–¶ï¸ Run Frontend

```bash
cd promptEval
npm install
npm start     # Runs on http://localhost:4200
```

---

## ğŸ›  2. Backend â€” Node.js + Express

### ğŸ§­ Features

- ğŸŒ RESTful APIs for authentication, templates, and evaluation
- ğŸª Secure cookies with JWT for login sessions
- ğŸŒ CORS configuration for Angular frontend
- ğŸ“¡ Connects to MongoDB Atlas for persistent storage

### â–¶ï¸ Run Backend

```bash
cd backend
npm install
npm run dev   # Runs on http://localhost:10000
```

### âš™ï¸ Required `.env` (backend/.env)

```env
PORT=10000
MONGO_URI=<your_mongodb_atlas_uri>
JWT_SECRET=<your_secret_key>
```

> Make sure your IP is whitelisted in MongoDB Atlas.

---

## ğŸ¤– 3. AI Model â€” Python (scikit-learn)

The AI model scores prompts based on clarity, context, relevance, specificity, and creativity.
It uses a pre-trained Ridge Regressor to generate a composite score between 0 and 10.

### ğŸ“‚ Key Files

- `scoring_artifacts/infer_ridge.py` â€” Loads the model & performs inference
- `scoring_artifacts/regressor.joblib` â€” Serialized trained model
- `scoring_artifacts/meta.json` â€” Metadata used during inference

### â–¶ï¸ Run Model

```bash
cd AI_model
python3 -m venv my_venv
source my_venv/bin/activate
pip install -r requirements.txt

# Run the Flask or FastAPI model server (not shown, but typically)
python scoring_artifacts/infer_ridge.py
```

---

## ğŸ”— Integration Flow

1. ğŸ§‘â€ğŸ’» User logs in through Angular frontend.
2. ğŸŒ Angular calls backend API at `http://localhost:10000/api/v1/...`.
3. ğŸ§  When evaluating a prompt, backend forwards the text to the Python model endpoint (e.g., `http://localhost:5000/evaluate`).
4. ğŸ“Š Model returns a JSON score, which backend processes and returns to frontend.
5. ğŸ§¾ Frontend displays scores and updates templates dynamically.

---

## ğŸ³ Optional â€” Dockerize AI Model

The `AI_model/DOCKERFILE` can be used to containerize the Python scoring service:

```bash
cd AI_model
docker build -t prompt-eval-model .
docker run -p 5000:5000 prompt-eval-model
```

---

## ğŸš€ Deployment Notes

- Frontend can be deployed on **Vercel** or **Netlify**.
- Backend can be deployed on **Render**, **Railway**, or **Azure App Service**.
- Python model can be deployed on **Render**, **AWS Lambda**, or **Docker + VM**.
- Ensure proper **CORS** configuration between deployed URLs.
- Use environment variables for API keys and secrets.

---

## ğŸ“ TODO / Future Work

- âœ… Fix Angular clipboard copy functionality
- ğŸ”„ Improve error handling for Gemini API errors
- ğŸ§  Add fine-tuning options for the scoring model
- ğŸ“ˆ Add analytics dashboard for prompt performance
- ğŸŒ Add multi-user role support

---

## ğŸ‘¨â€ğŸ’» Authors

- **George Njunge** â€” Full-stack Developer & AI Engineer

---

## ğŸ“„ License

MIT License Â© 2025 Prompt-Eval Project
    